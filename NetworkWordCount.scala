package com.test

import org.apache.spark.{SparkConf, SparkContext}
import org.apache.spark.storage.StorageLevel
import org.apache.spark.streaming.{Seconds, StreamingContext}
import scala.util.control.NonFatal

/**

NOTE: If I don't push any data to socket, the job continues to run. No
      exceptions but as soon as I push some data (nc -lk 9999), I see
      exceptions in executor + driver logs and the same exception is
      thrown by awaitTermination and the job exits and stops further
      processing.

*/
object NetworkWordCount {
  def main(args: Array[String]) {
    val sparkConf = new SparkConf().setAppName("NetworkWordCount")
    val sc = new StreamingContext(sparkConf, Seconds(1))
    val results = runJob(sc)
    println("Results: " +results)
  }

  def runJob(ssc: StreamingContext): Any = {
    // Create the context with a 1 second batch size
    //val sparkConf = new SparkConf().setAppName("NetworkWordCount")
    //val ssc = new StreamingContext(sparkConf, Seconds(1))

    // Create a socket stream on target ip:port and count the
    // words in input stream of \n delimited text (eg. generated by 'nc')
    // Note that no duplication in storage level only for running locally.
    // Replication necessary in distributed scenario for fault tolerance.
    val lines = ssc.socketTextStream("localhost", 9999, StorageLevel.MEMORY_AND_DISK_SER)
    val words = lines.flatMap(_.split(" "))
    val wordCounts = words.map {
      x => (x, 1)
    }.reduceByKey {
      (a ,b) =>
        1/0
        (a  + b)
    }
    wordCounts.print()
    ssc.start()
    ssc.awaitTermination()
     try {
       ssc.awaitTermination()
     } catch {
       case e: Throwable => println("Exception caught", e)
     }
  }
}
